---
title: "Home Credit EDA Modeling Workbook"
author: "Group 8: Jade Gosar, Karson Eilers, Paula Soutostefani"
date: "2023-07-01"
output: 
  html_document:
    toc:true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
#Warnings disabled because of NB zero probability classification warnings
```

#Introduction

TODO: ADD INTRO AND BACKGROUND


## SETUP

```{r packages}
#install.packages("randomForest")
#install.packages("rminer")

#load packages
library(tidyverse)
library(caret)
library(naivebayes)
library(readr)
library(dplyr)
library(rpart)
library(rminer)
library(randomForest)
library(pROC)
library(MASS)

```


```{r data import}
#Imports cleaned training and testing set containing relevant varaibles and no na values
#see 'data_consolidation_script.R' for full details

#The training set is a product of the application_train.csv set and two values from bureau.csv
# UPDATE - to improve models, more variables have been added.
training_set <- read_csv('clean_training_data.csv')
#note - the updated cleaned training set now has 28 variables and 59,441 observations. Prev file had 263,480 observations instead fo the 307511 in the original file
testing_set <- read_csv('clean_testing_data.csv')
#note - the cleaned testing set now has 27 variables (everything except for TARGET) and 9,763 observations. Prev file had 42,299 observations instead of the 48,744 in the original data

```
##Observation on cleaning and TARGET value frequency.
Group 8 is concerned about the effect of cleaning on the already imbalanced target classification. The group set a tolerance threshold of a 10% change in the TARGET variable. If the cleaning resulted in a disproportionate relative increase or decrease in the target variable frequency, the group would reevaluate cleaning methods.

The cleaning methods used resulted in a 4.24% reduction in the TARGET variable, well below the group's threshold. 

```{r Target variable testing}
## Group note - you don't need to import the raw data back in. We'll just uncomment this in the final submissiont to demonstrate the change percentage in the TARGET variable

#raw_data_train <- read_csv("application_train.csv")

#(mean(raw_data_train$TARGET) - mean(training_set$TARGET))/mean(raw_data_train$TARGET)

```


```{r data formatting}
#Some of the variables need to be treated as factors for the subsequebnt modeling steps

#Let's filter the characters first
testing_set <- testing_set %>% mutate(across(where(is.character), as.factor))
training_set <- training_set %>% mutate(across(where(is.character), as.factor))

#we should factor the Target variable for classification approaches, too.
training_set$TARGET <- as.factor(training_set$TARGET)

#There appears to be one anomaly in the DAYS_EMPLOYED Values; a very large positive number. 
training_set %>%
   ggplot(aes(DAYS_EMPLOYED)) + geom_boxplot()

testing_set %>%
   ggplot(aes(DAYS_EMPLOYED)) + geom_boxplot()


summary(testing_set$DAYS_EMPLOYED)
summary(training_set$DAYS_EMPLOYED)

#The anomoly occurs in both training and testing. It must be a mis entry as it's impossible to work 365,243 days in a human lifetime. We will remove it from both sets.
training_set <- training_set %>%
  filter(DAYS_EMPLOYED <= 0)

summary(training_set$DAYS_EMPLOYED)

testing_set <- testing_set %>%
  filter(DAYS_EMPLOYED <= 0)

summary(training_set$DAYS_EMPLOYED)

#DAYS_EMPLOYED and DAYS_CREDIT are both negative values, since they are past date - current date. Let's make them absolute values to be easier to interpret. 
training_set$DAYS_EMPLOYED <- abs(training_set$DAYS_EMPLOYED)
testing_set$DAYS_EMPLOYED <- abs(testing_set$DAYS_EMPLOYED)
training_set$DAYS_CREDIT <- abs(training_set$DAYS_CREDIT)
testing_set$DAYS_CREDIT <- abs(testing_set$DAYS_CREDIT)

#Continuous variable distributions
training_set %>%
  ggplot(aes(x=AMT_INCOME_TOTAL)) + geom_density() #right skew

summary(training_set$DAYS_EMPLOYED)

training_set %>%
  ggplot(aes(x=DAYS_CREDIT)) + geom_density() #left skew


#using scale to normalize days_employed variable
#training_set$DAYS_EMPLOYED <- scale(training_set$DAYS_EMPLOYED)
#testing_set$DAYS_EMPLOYED <- scale(testing_set$DAYS_EMPLOYED)

#using scale to normalize days_credit
#training_set$DAYS_CREDIT <- scale(training_set$DAYS_CREDIT)
#testing_set$DAYS_CREDIT <- scale(testing_set$DAYS_CREDIT)

#using scale to normalize AMT_INCOME
#training_set$AMT_INCOME_TOTAL <- log(training_set$AMT_INCOME_TOTAL)
#testing_set$AMT_INCOME_TOTAL <- log(testing_set$AMT_INCOME_TOTAL)

#using scale to normalize AMT_CREDIT
#training_set$AMT_CREDIT <- log(training_set$AMT_CREDIT)
#testing_set$AMT_CREDIT <- log(testing_set$AMT_CREDIT)

#using scale to normalize AMT_ANNUITY
#training_set$AMT_ANNUITY <- log(training_set$AMT_ANNUITY)
#testing_set$AMT_ANNUITY <- log(testing_set$AMT_ANNUITY)

#remove sk_curr_ID to avoid incidentally using it as a predictor
training_set <- training_set[-c(1)]
testing_set <- testing_set[-c(1)]

```
Changing character columns into factor variables to add as dummy variables into analysis

```{r}
# Select character columns that contain categorical data to turn into factor variables
columns <- c("NAME_INCOME_TYPE", "NAME_EDUCATION_TYPE")

#/-------> Commented out - redundent section ---------->
# Loop over the columns selected and convert them to factors
#for (column in columns) {
#  training_set[[column]] <- factor(training_set[[column]])
#}

#str(training_set)
```

Create dummy variable for categorical variables
```{r}
# create dummy variables for categorical variables
dummies <- model.matrix(~ NAME_INCOME_TYPE + NAME_EDUCATION_TYPE + 0, data = training_set)

# add the dummy variables to the original data frame
training_set_w_dummies <- cbind(training_set, dummies)

# rename the dummy variable columns
#ERRORS
colnames(training_set_w_dummies)[12:22] <- c("Bussinessman", "Commercial_Associate", "Maternity_Leave", "Pensioner", "State_Servant", "Student", "Working_Class", "Higher_Education", "Incomplete_Higher_Education", "Lower_Seconday_Education", "Seconday_Secondary_Special")

#ERRORS
# check the result
training_set_w_dummies[,c(6, 9, 12:22)]
```

Turn Yes/No columns into 1's and 0's to be used in modeling
```{r}
training_set_w_dummies$FLAG_OWN_CAR <- as.integer(training_set_w_dummies$FLAG_OWN_CAR == "Y")
training_set_w_dummies$FLAG_OWN_REALTY <- as.integer(training_set_w_dummies$FLAG_OWN_REALTY == "Y")
training_set_w_dummies$TARGET <- as.integer(training_set_w_dummies$TARGET == "1")
```


## Partitions
We will need to partition the training set into (at least) two partitions - one for training the data and one for testing. We need to test on a training partition before deploying the model to the formal testing set to measure accuracy (testing_set doesn't have the TARGET variable)

This code will partition the training set into two: t_train and t_test. We will set the testing_set aside for now. use that at the end for final model predictions. 

```{r partitions}
set.seed(234)

#creates a training subset of the training data with 70% of the data
t_train_index <- createDataPartition(training_set$TARGET, p = 0.7, list=FALSE)
t_train_index_dummies <- createDataPartition(training_set_w_dummies$TARGET, p = 0.7, list=FALSE)


t_train <- training_set[t_train_index,]
t_test <- training_set[-t_train_index,]

t_train_dummies <- training_set_w_dummies[t_train_index,]
t_test_dummies <- training_set_w_dummies[-t_train_index,]

#check data
summary(t_train)
summary(t_test)

#check for relative frequency of Target in t_train and t_test
t_train %>%
  group_by(TARGET) %>%
  summarise(percent = n()/nrow(.))

t_test %>%
  group_by(TARGET) %>%
  summarise(percent = n()/nrow(.))

```

```{r upsample_option}
#up sample target class to address class imbalance
training_sample <- upSample(t_train, t_train$TARGET)

```


Looks like we are set! 
Note from Karson: I didn't sample to address classification bias or standardize the values with wide variance like income or loan amount. Tweaks like those may improve your model performance, but I wanted to give you both the options to try different approaches. Feel free to modify the data how you see fit. 
<-------------------START CODING MODELS HERE--------------->

<-------------------Decision Trees & Random Forest -------------->
```{r}
head(t_train)
head(t_test)
dim(t_train)
dim(t_test)
```

```{r}
tree_mod <- rpart(TARGET ~.,
                  data = t_train[-c(1:2, 6, 9)])

tree_mod
```

```{r}
rf_mod_default <- randomForest(TARGET ~.,
                               data = t_train)
rf_mod_default
```

```{r}
#Evaluate performance against the training dataset and then the test dataset
rf_def_predict_train <- predict(rf_mod_default, t_train)
mmetric(t_train$TARGET, rf_def_predict_train, metric = "ACC", "TPR", "PRECISION", "F1")
```


```{r}
rf_def_predict_test <- predict(rf_mod_default, t_test)
mmetric(t_test$TARGET, rf_def_predict_test, metric = "ACC", "TPR", "PRECISION", "F1")
```


Plot Error Rate v Number of Trees in random forest
```{r}
oob_error <- rf_mod_default$err.rate[,1]
plot_dat <- cbind.data.frame(rep(1:length(oob_error)), oob_error)
names(plot_dat) <- c("trees", "oob_error")

g_1 <- ggplot(plot_dat, aes(x = trees, y = oob_error)) +
  geom_point(alpha = 0.5, color = "blue") +
  theme_bw() +
  geom_smooth() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  labs(title = "Error Rate v Number of Trees", x = "Number of Trees", y = "Error Rate")
```

```{r}
rf_mod <- randomForest(TARGET ~.,
                       data = t_train,
                       ntree = 500)

rf_mod
```

<--------------------------Naive Bayes Classification----------------------->
```{r nb_prep}
#re-factor TARGET variable
t_train$TARGET <- as.factor(t_train$TARGET)
t_test$TARGET <- as.factor(t_test$TARGET)
training_sample$TARGET <- as.factor(training_sample$TARGET)


```

```{r nb1}
#laplace smoothing @ value of 1 (default = 0), laplace smooting helps address zero probability issues
#Using kernel trick to address issue of dimensionality.

nb1 <- naive_bayes(TARGET ~ ., data=t_train, laplace=1, usekernel=T)
plot(nb1)

nb1_predict_class <- predict(nb1, newdata = t_train)
cm <- confusionMatrix(data = nb1_predict_class, reference = t_train$TARGET)
print(cm)
```

```{r nb2}

# set up 10-fold cross validation procedure
ctrl10x <- trainControl(
  method = "cv", 
  number = 10
  )

# train model using caret, 10x cross validation
nb2 <- train(
  x = t_train[,2:8],
  y = t_train$TARGET,
  method = "nb",
  trControl = ctrl10x
  )

nb2_predict_class <- predict(nb2, newdata = t_train)

nb2_predict_test <- predict(nb2, newdata = t_test)

cm <- confusionMatrix(data = nb2_predict_class, reference = t_train$TARGET)
cm_test <- confusionMatrix(data <- nb2_predict_test, reference = t_test$TARGET)

print(cm_test)
```

NB 3 uses upsampling to test effectiveness
```{r nb3}
ctrl10x <- trainControl(
  method = "cv", 
  number = 10
  )

# train model using caret, 10x cross validation
nb3 <- train(
  x = training_sample[,2:27],
  y = training_sample$TARGET,
  method = "nb",
  trControl = ctrl10x
  )

nb3_predict_class <- predict(nb3, newdata = training_sample)

nb3_predict_test <- predict(nb3, newdata = t_test)


cm <- confusionMatrix(data = nb3_predict_class, reference = training_sample$TARGET)
cm_test <- confusionMatrix(data <- nb3_predict_test, reference = t_test$TARGET)

print(cm)
print(cm_test)

```


<---------------------------- Logistic Regression ------------------------->

# Creation of a summary table of conditional TARGET rates by Income Type and Education Type:
```{r}

t_train %>% 
  group_by(NAME_INCOME_TYPE, NAME_EDUCATION_TYPE) %>% 
  summarize(TARGET = sum(TARGET=="1")) %>%
  arrange(desc(TARGET))

```


```{r}
# Creation of a summary table of conditional TARGET rates by Total Income Amount:

# I need help here in grouping income amounts in maybe 5 different levels and running this by level type. 

t_train %>% 
  group_by(AMT_INCOME_TOTAL,) %>% 
  summarize(TARGET = sum(TARGET=="1")) %>%
  arrange(desc(TARGET))


# Creation of a summary table of conditional TARGET rates by Amount Annuity:

t_train %>% 
  group_by(AMT_ANNUITY,) %>% 
  summarize(TARGET = sum(TARGET=="1")) %>%
  arrange(desc(TARGET))

# Creation of a summary table of conditional TARGET rates by Amount Credit:

t_train %>% 
  group_by(AMT_CREDIT,) %>% 
  summarize(TARGET = sum(TARGET=="1")) %>%
  arrange(desc(TARGET))

```



```{r}
# Calculating mean of TARGET rate in training and testing sets::

mean(t_train$TARGET==1)
mean(t_train$TARGET==0)

mean(t_test$TARGET==1)
mean(t_test$TARGET==0)
```

In this step I started to create the logistic models and run validation / accuracy in order to identify which of the Logistic Regression Models would work the best in terms of having the strongest predictors of TARGET. I started with fewer predictors and increased one by one comparing the AIC, deviance, accuracy, and ROC Curve.




```{r}
# Model 1: Simple Logistic Regression Model: Target explained by Total Income Amount:

model1 <- glm(TARGET ~ AMT_INCOME_TOTAL, data = t_train, family = binomial)
summary(standardize(model1))

invlogit <- function(x) exp(x)/(1 + exp(x))

invlogit(-2.2 + 0.0 * 0) %>%
  round(2)

#  -2.2  represents  log odds of having issues in payment (TARGET = 1) when Total Income amount = 0 

#  Average Income Amount:

mean(t_train$AMT_INCOME_TOTAL)

# Performing logistic regression of Target explained by Average Total Income Amount:

invlogit <- function(x) exp(x)/(1 + exp(x))

invlogit(-2.2 + 0.0 * 177929.5) %>%
  round(2)

```


```{r}
# Calculating Deviance for Model1:

glm(TARGET ~ AMT_INCOME_TOTAL, data = t_train, family = binomial)$deviance
```
Model1: 
AIC= 86485 - High AIC as a result of having only one predictor for Model1
Deviance: 86480.83

```{r}
# Model2: Simple Logistic Regression Model incorporating more predictors: Target explained by Total Income Amount + Total CREDIT Amount + Total Annuity Amount:

model2 <- glm(TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY, data = t_train, family = binomial)
summary(standardize(model2))

```
```{r}
# Calculating Deviance for Model2:

glm(TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY, data = t_train, family = binomial)$deviance
```


Model2:
AIC= 86284, which is 201 AIC points less than Model1 (86485), showing that this is a more effective model in predicting the target Variable.
Deviance = 86275.68



```{r}

# Model3: Simple Logistic Regression Model incorporating more predictors: Target explained by AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT. 

model3 <- glm(TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT, data = t_train, family = binomial)
summary(standardize(model3))

```
```{r}
# Calculating Deviance for Model3:

glm(TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT, data = t_train, family = binomial)$deviance
```

Model3:
AIC= 84843, which is 1441 AIC points less than Model2 (86284), and 1642 AIC points less than Model1(86485) - showing that this is a more effective model in predicting the target Variable.
Deviance: 84831.29



```{r}
# Model4:  Logistic Regression Model incorporating more predictors: Target explained by AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE. 

model4 <- glm(TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE , data = t_train, family = binomial)
summary(standardize(model4))

```
```{r}
# Calculating Deviance for Model4:

glm(TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE , data = t_train, family = binomial)$deviance
```


Model4:
AIC= 84133, which is 710 AIC points less than Model3 (84843), and 2151 AIC points less than Model2(86284) - showing that this is a more effective model in predicting the target Variable.
Deviance: 84101.18

```{r}
# Creation of Confusion Matrix (predicted/actual) for Model4:

confusionMatrix(ifelse(predict(model4, newdata=t_train, type="response")> .5, 1, 0),
                t_train$TARGET)


```


```{r}
# AUC and Roc Curve for Model4:

library(pROC)

roc(t_train$TARGET, 
    predict(model4, type = "response"), 
    plot=T, 
    add= T,
    col=2)

```



```{r}
# Model4 Residual Plot: Binnedplot:

binnedplot(fitted(model4), 
           t_train$TARGET - fitted(model4))
```


```{r}
# Fitting a KNN model of TARGET using the same predictor variables for model4.

set.seed(1234)
  (knn_model <- train(factor(TARGET) ~ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + DAYS_EMPLOYED + DAYS_CREDIT + NAME_INCOME_TYPE + NAME_EDUCATION_TYPE, 
                    data=t_train, 
                    preProcess=c("center", "scale"),
                    method="knn"))

AIC(knn_model)
```










